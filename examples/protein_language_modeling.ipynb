{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af5d6f2e",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install ðŸ¤— Transformers as well as some other libraries. Uncomment the following cell and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5bf8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install transformers evaluate datasets requests pandas sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e71a3f",
   "metadata": {},
   "source": [
    "If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n",
    "\n",
    "To be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.\n",
    "\n",
    "First you have to store your authentication token from the Hugging Face website (sign up here if you haven't already!) then execute the following cell and input your username and password:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b8526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8b2712",
   "metadata": {},
   "source": [
    "Then you need to install Git-LFS. Uncomment the following instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e8f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0749e1",
   "metadata": {},
   "source": [
    "# Fine-Tuning Protein Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d81db83",
   "metadata": {},
   "source": [
    "In this notebook, we're going to do some transfer learning to fine-tune some large, pre-trained protein language models on tasks of interest. If that sentence feels a bit intimidating to you, don't panic - there's also a blog post (link when it's done!) that explains the concepts here in much more detail. We highly recommend starting there if you're a biologist who isn't familiar with the state-of-the-art in language modeling (or if you're a machine learning engineer who isn't quite sure what a protein is!)\n",
    "\n",
    "The specific model we're going to use is ESM-2, which is the state-of-the-art protein language model at the time of writing (September 2022). The citation for this model is [Lin et al, 2022](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1).\n",
    "\n",
    "There are several ESM-2 checkpoints with differing model sizes. Larger models will generally have better accuracy, but they require more GPU memory and will take much longer to train. The available ESM-2 checkpoints (at time of writing) are:\n",
    "\n",
    "| Checkpoint name | Num layers | Num parameters |\n",
    "|------------------------------|----|----------|\n",
    "| `esm2_t48_15B_UR50D`         | 48 | 15B     |\n",
    "| `esm2_t36_3B_UR50D`          | 36 | 3B      | \n",
    "| `esm2_t33_650M_UR50D`        | 33 | 650M    | \n",
    "| `esm2_t30_150M_UR50D`        | 30 | 150M    | \n",
    "| `esm2_t12_35M_UR50D`         | 12 | 35M     | \n",
    "| `esm2_t6_8M_UR50D`           | 6  | 8M      | \n",
    "\n",
    "Note that the larger checkpoints may be very difficult to train without a large cloud GPU like an A100 or H100, and the largest 15B parameter checkpoint will probably be impossible to train on **any** single GPU! We will use the `esm2_t30_150M_UR50D` checkpoint for this notebook, which should train on any Colab instance or modern GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32e605a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"Rocketknight1/esm2_t12_35M_UR50D\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e6ac19",
   "metadata": {},
   "source": [
    "# Sequence classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb400c",
   "metadata": {},
   "source": [
    "One of the most common tasks you can perform with a language model is **sequence classification**. In sequence classification, we classify an entire protein into a category, from a list of two or more possibilities. There's no limit on the number of categories you can use, or the specific problem you choose, as long as it's something the model could in theory infer from the raw protein sequence. To keep things simple for this example, though, let's try classifying proteins by their cellular localization - given their sequence, can we predict if they're going to be found in the cytosol (the fluid inside the cell) or embedded in the cell membrane?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc122f",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c91d394",
   "metadata": {},
   "source": [
    "In this section, we're going to gather some training data from open web databases. Our goal is to create a pair of lists: `sequences` and `labels`. `sequences` will be a list of protein sequences, which will just be strings like \"MNKL...\", where each letter represents a single amino acid in the complete protein. `labels` will be a list of the category for each sequence. The categories will just be integers, with 0 representing the first category, 1 representing the second and so on. In other words, if `sequences[i]` is a protein sequence then `labels[i]` should be its corresponding category. These will form the **training data** we're going to use to teach the model the task we want it to do.\n",
    "\n",
    "If you're adapting this notebook for your own use, this will probably be the main section you want to change! You can do whatever you want here, as long as you create those two lists by the end of it. If you want to follow along with this example, though, first we'll need to `import requests` and set up our query to UniProt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c718ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "query_url =\"https://rest.uniprot.org/uniprotkb/stream?compressed=true&fields=accession%2Csequence%2Ccc_subcellular_location&format=tsv&query=%28%28organism_id%3A9606%29%20AND%20%28reviewed%3Atrue%29%20AND%20%28length%3A%5B100%20TO%201000%5D%29%29\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2edc14",
   "metadata": {},
   "source": [
    "This query URL might seem mysterious, but it isn't! To get it, we searched for `(organism_id:9606) AND (reviewed:true) AND (length:[100 TO 1000])` on UniProt to get a list of reasonably-sized human proteins,\n",
    "then selected 'Download', and set the format to TSV and the columns to `Sequence` and `Subcellular location [CC]`, since those contain the data we care about for this task.\n",
    "\n",
    "Once that's done, selecting `Generate URL for API` gives you a URL you can pass to Requests. Alternatively, if you're not on Colab you can just download the data through the web interface and open the file locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd03ef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot_request = requests.get(query_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7217b77",
   "metadata": {},
   "source": [
    "To get this data into Pandas, we use a `BytesIO` object, which Pandas will treat like a file. If you downloaded the data as a file you can skip this bit and just pass the filepath directly to `read_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2c05017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Subcellular location [CC]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A087X1C5</td>\n",
       "      <td>MGLEALVPLAMIVAIFLLLVDLMHRHQRWAARYPPGPLPLPGLGNL...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A0B4J2F2</td>\n",
       "      <td>MVIMSEFSADPAGQGQGQQKPLRVGFYDIERTLGKGNFAVVKLARH...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A0K2S4Q6</td>\n",
       "      <td>MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...</td>\n",
       "      <td>SUBCELLULAR LOCATION: [Isoform 1]: Membrane {E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A1B0GTW7</td>\n",
       "      <td>MLLLLLLLLLLPPLVLRVAASRCLHDETQKSVSLLRPPFSQLPSKS...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000255}; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A5B9</td>\n",
       "      <td>DLKNVFPPKVAVFEPSEAEISHTQKATLVCLATGFYPDHVELSWWV...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cell membrane {ECO:00003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17258</th>\n",
       "      <td>Q9NZ38</td>\n",
       "      <td>MAFPGQSDTKMQWPEVPALPLLSSLCMAMVRKSSALGKEVGRRSEG...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17259</th>\n",
       "      <td>Q9UF83</td>\n",
       "      <td>MRRPSTASLTRTPSRASPTRMPSRASLKMTPFRASLTKMESTALLR...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17260</th>\n",
       "      <td>Q9UFV3</td>\n",
       "      <td>MAETYRRSRQHEQLPGQRHMDLLTGYSKLIQSRLKLLLHLGSQPPV...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17261</th>\n",
       "      <td>X6R8D5</td>\n",
       "      <td>MGRKEHESPSQPHMCGWEDSQKPSVPSHGPKTPSCKGVKAPHSSRP...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17262</th>\n",
       "      <td>X6R8R1</td>\n",
       "      <td>MGVVLSPHPAPSRREPLAPLAPGTRPGWSPAVSGSSRSALRPSTAG...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17263 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Entry                                           Sequence  \\\n",
       "0      A0A087X1C5  MGLEALVPLAMIVAIFLLLVDLMHRHQRWAARYPPGPLPLPGLGNL...   \n",
       "1      A0A0B4J2F2  MVIMSEFSADPAGQGQGQQKPLRVGFYDIERTLGKGNFAVVKLARH...   \n",
       "2      A0A0K2S4Q6  MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...   \n",
       "3      A0A1B0GTW7  MLLLLLLLLLLPPLVLRVAASRCLHDETQKSVSLLRPPFSQLPSKS...   \n",
       "4          A0A5B9  DLKNVFPPKVAVFEPSEAEISHTQKATLVCLATGFYPDHVELSWWV...   \n",
       "...           ...                                                ...   \n",
       "17258      Q9NZ38  MAFPGQSDTKMQWPEVPALPLLSSLCMAMVRKSSALGKEVGRRSEG...   \n",
       "17259      Q9UF83  MRRPSTASLTRTPSRASPTRMPSRASLKMTPFRASLTKMESTALLR...   \n",
       "17260      Q9UFV3  MAETYRRSRQHEQLPGQRHMDLLTGYSKLIQSRLKLLLHLGSQPPV...   \n",
       "17261      X6R8D5  MGRKEHESPSQPHMCGWEDSQKPSVPSHGPKTPSCKGVKAPHSSRP...   \n",
       "17262      X6R8R1  MGVVLSPHPAPSRREPLAPLAPGTRPGWSPAVSGSSRSALRPSTAG...   \n",
       "\n",
       "                               Subcellular location [CC]  \n",
       "0      SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  \n",
       "1                                                    NaN  \n",
       "2      SUBCELLULAR LOCATION: [Isoform 1]: Membrane {E...  \n",
       "3      SUBCELLULAR LOCATION: Membrane {ECO:0000255}; ...  \n",
       "4      SUBCELLULAR LOCATION: Cell membrane {ECO:00003...  \n",
       "...                                                  ...  \n",
       "17258                                                NaN  \n",
       "17259                                                NaN  \n",
       "17260                                                NaN  \n",
       "17261                                                NaN  \n",
       "17262                                                NaN  \n",
       "\n",
       "[17263 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import pandas\n",
    "\n",
    "bio = BytesIO(uniprot_request.content)\n",
    "\n",
    "df = pandas.read_csv(bio, compression='gzip', sep='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcdf34b",
   "metadata": {},
   "source": [
    "Nice! Now we have some proteins and their subcellular locations. Let's start filtering this down. First, let's ditch the columns without subcellular location information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31d87663",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()  # Drop proteins with missing columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d1af5c",
   "metadata": {},
   "source": [
    "Now we'll make one dataframe of proteins that contain `cytosol` or `cytoplasm` in their subcellular localization column, and a second that mentions the `membrane` or `cell membrane`. To ensure we don't get overlap, we ensure each dataframe only contains proteins that don't match the other search term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c831bb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "cytosolic = df['Subcellular location [CC]'].str.contains(\"Cytosol\") | df['Subcellular location [CC]'].str.contains(\"Cytoplasm\")\n",
    "membrane = df['Subcellular location [CC]'].str.contains(\"Membrane\") | df['Subcellular location [CC]'].str.contains(\"Cell membrane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f41139a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Subcellular location [CC]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A0MZ66</td>\n",
       "      <td>MNSSDEEKQLQLITSLKEQAIGEYEDLRAENQKTKEKCDKIRQERD...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Perikaryon {ECO:0000250|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>A1E959</td>\n",
       "      <td>MKIIILLGFLGATLSAPLIPQRLMSASNSNELLLNLNNGQLLPLQL...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Secreted {ECO:0000250|Un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>A1L4K1</td>\n",
       "      <td>MEEESGEELGLDRSTPKDFHFYHMDLYDSEDRLHLFPEENTRMRKV...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Nucleus {ECO:0000250|Uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>A1X283</td>\n",
       "      <td>MPPRRSIVEVKVLDVQKRRVPNKHYVYIIRVTWSSGSTEAIYRRYS...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm {ECO:0000250}....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>A1XBS5</td>\n",
       "      <td>MMRRTLENRNAQTKQLQTAVSNVEKHFGELCQIFAAYVRKTARLRD...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm {ECO:0000269|P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16862</th>\n",
       "      <td>Q96M43</td>\n",
       "      <td>MVVSADPLSSERAEMNILEINQELRSQLAESNQQFRDLKEKFLITQ...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm {ECO:0000305}.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16900</th>\n",
       "      <td>Q9BYD9</td>\n",
       "      <td>MNHCQLPVVIDNGSGMIKAGVAGCREPQFIYPNIIGRAKGQSRAAQ...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm, cytoskeleton ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16943</th>\n",
       "      <td>Q9NPB0</td>\n",
       "      <td>MEQRLAEFRAARKRAGLAAQPPAASQGAQTPGEKAEAAATLKAAPG...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasmic vesicle memb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16955</th>\n",
       "      <td>Q9NUJ7</td>\n",
       "      <td>MGGQVSASNSFSRLHCRNANEDWMSALCPRLWDVPLHHLSIPGSHD...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm {ECO:0000269|P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16966</th>\n",
       "      <td>Q9P2W6</td>\n",
       "      <td>MGRTWCGMWRRRRPGRRSAVPRWPHLSSQSGVEPPDRWTGTPGWPS...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3954 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entry                                           Sequence  \\\n",
       "15     A0MZ66  MNSSDEEKQLQLITSLKEQAIGEYEDLRAENQKTKEKCDKIRQERD...   \n",
       "25     A1E959  MKIIILLGFLGATLSAPLIPQRLMSASNSNELLLNLNNGQLLPLQL...   \n",
       "30     A1L4K1  MEEESGEELGLDRSTPKDFHFYHMDLYDSEDRLHLFPEENTRMRKV...   \n",
       "31     A1X283  MPPRRSIVEVKVLDVQKRRVPNKHYVYIIRVTWSSGSTEAIYRRYS...   \n",
       "32     A1XBS5  MMRRTLENRNAQTKQLQTAVSNVEKHFGELCQIFAAYVRKTARLRD...   \n",
       "...       ...                                                ...   \n",
       "16862  Q96M43  MVVSADPLSSERAEMNILEINQELRSQLAESNQQFRDLKEKFLITQ...   \n",
       "16900  Q9BYD9  MNHCQLPVVIDNGSGMIKAGVAGCREPQFIYPNIIGRAKGQSRAAQ...   \n",
       "16943  Q9NPB0  MEQRLAEFRAARKRAGLAAQPPAASQGAQTPGEKAEAAATLKAAPG...   \n",
       "16955  Q9NUJ7  MGGQVSASNSFSRLHCRNANEDWMSALCPRLWDVPLHHLSIPGSHD...   \n",
       "16966  Q9P2W6  MGRTWCGMWRRRRPGRRSAVPRWPHLSSQSGVEPPDRWTGTPGWPS...   \n",
       "\n",
       "                               Subcellular location [CC]  \n",
       "15     SUBCELLULAR LOCATION: Perikaryon {ECO:0000250|...  \n",
       "25     SUBCELLULAR LOCATION: Secreted {ECO:0000250|Un...  \n",
       "30     SUBCELLULAR LOCATION: Nucleus {ECO:0000250|Uni...  \n",
       "31     SUBCELLULAR LOCATION: Cytoplasm {ECO:0000250}....  \n",
       "32     SUBCELLULAR LOCATION: Cytoplasm {ECO:0000269|P...  \n",
       "...                                                  ...  \n",
       "16862     SUBCELLULAR LOCATION: Cytoplasm {ECO:0000305}.  \n",
       "16900  SUBCELLULAR LOCATION: Cytoplasm, cytoskeleton ...  \n",
       "16943  SUBCELLULAR LOCATION: Cytoplasmic vesicle memb...  \n",
       "16955  SUBCELLULAR LOCATION: Cytoplasm {ECO:0000269|P...  \n",
       "16966                   SUBCELLULAR LOCATION: Cytoplasm.  \n",
       "\n",
       "[3954 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cytosolic_df = df[cytosolic & ~membrane]\n",
    "cytosolic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be5c420e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Subcellular location [CC]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A0K2S4Q6</td>\n",
       "      <td>MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...</td>\n",
       "      <td>SUBCELLULAR LOCATION: [Isoform 1]: Membrane {E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A1B0GTW7</td>\n",
       "      <td>MLLLLLLLLLLPPLVLRVAASRCLHDETQKSVSLLRPPFSQLPSKS...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000255}; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A5B9</td>\n",
       "      <td>DLKNVFPPKVAVFEPSEAEISHTQKATLVCLATGFYPDHVELSWWV...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cell membrane {ECO:00003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A0AV02</td>\n",
       "      <td>MTQMSQVQELFHEAAQQDALAQPQPWWKTQLFMWEPVLFGTWDGVF...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A0FGR8</td>\n",
       "      <td>MTANRDAALSSHRHPGCAQRPRTPTFASSSQRRSAFGFDDGNFPGL...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cell membrane {ECO:00002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17131</th>\n",
       "      <td>Q6UWF5</td>\n",
       "      <td>MQIQNNLFFCCYTVMSAIFKWLLLYSLPALCFLLGTQESESFHSKA...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17203</th>\n",
       "      <td>Q8N8V8</td>\n",
       "      <td>MLLKVRRASLKPPATPHQGAFRAGNVIGQLIYLLTWSLFTAWLRPP...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17242</th>\n",
       "      <td>Q96N68</td>\n",
       "      <td>MQGQGALKESHIHLPTEQPEASLVLQGQLAESSALGPKGALRPQAQ...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17248</th>\n",
       "      <td>Q9H0A3</td>\n",
       "      <td>MMNNTDFLMLNNPWNKLCLVSMDFCFPLDFVSNLFWIFASKFIIVT...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000255}; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17250</th>\n",
       "      <td>Q9H354</td>\n",
       "      <td>MNKHNLRLVQLASELILIEIIPKLFLSQVTTISHIKREKIPPNHRK...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3582 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Entry                                           Sequence  \\\n",
       "2      A0A0K2S4Q6  MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...   \n",
       "3      A0A1B0GTW7  MLLLLLLLLLLPPLVLRVAASRCLHDETQKSVSLLRPPFSQLPSKS...   \n",
       "4          A0A5B9  DLKNVFPPKVAVFEPSEAEISHTQKATLVCLATGFYPDHVELSWWV...   \n",
       "5          A0AV02  MTQMSQVQELFHEAAQQDALAQPQPWWKTQLFMWEPVLFGTWDGVF...   \n",
       "10         A0FGR8  MTANRDAALSSHRHPGCAQRPRTPTFASSSQRRSAFGFDDGNFPGL...   \n",
       "...           ...                                                ...   \n",
       "17131      Q6UWF5  MQIQNNLFFCCYTVMSAIFKWLLLYSLPALCFLLGTQESESFHSKA...   \n",
       "17203      Q8N8V8  MLLKVRRASLKPPATPHQGAFRAGNVIGQLIYLLTWSLFTAWLRPP...   \n",
       "17242      Q96N68  MQGQGALKESHIHLPTEQPEASLVLQGQLAESSALGPKGALRPQAQ...   \n",
       "17248      Q9H0A3  MMNNTDFLMLNNPWNKLCLVSMDFCFPLDFVSNLFWIFASKFIIVT...   \n",
       "17250      Q9H354  MNKHNLRLVQLASELILIEIIPKLFLSQVTTISHIKREKIPPNHRK...   \n",
       "\n",
       "                               Subcellular location [CC]  \n",
       "2      SUBCELLULAR LOCATION: [Isoform 1]: Membrane {E...  \n",
       "3      SUBCELLULAR LOCATION: Membrane {ECO:0000255}; ...  \n",
       "4      SUBCELLULAR LOCATION: Cell membrane {ECO:00003...  \n",
       "5      SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  \n",
       "10     SUBCELLULAR LOCATION: Cell membrane {ECO:00002...  \n",
       "...                                                  ...  \n",
       "17131  SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  \n",
       "17203  SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  \n",
       "17242  SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  \n",
       "17248  SUBCELLULAR LOCATION: Membrane {ECO:0000255}; ...  \n",
       "17250  SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  \n",
       "\n",
       "[3582 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "membrane_df = df[membrane & ~cytosolic]\n",
    "membrane_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e8cea6",
   "metadata": {},
   "source": [
    "We're almost done! Now, let's make a list of sequences from each df and generate the associated labels. We'll use `0` as the label for cytosolic proteins and `1` as the label for membrane proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "023ec31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cytosolic_sequences = cytosolic_df[\"Sequence\"].tolist()\n",
    "cytosolic_labels = [0 for protein in cytosolic_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0e7318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "membrane_sequences = membrane_df[\"Sequence\"].tolist()\n",
    "membrane_labels = [1 for protein in membrane_sequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4bbda2",
   "metadata": {},
   "source": [
    "Now we can concatenate these lists together to get the `sequences` and `labels` lists that will form our final training data. Don't worry - they'll get shuffled during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dec7a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = cytosolic_sequences + membrane_sequences\n",
    "labels = cytosolic_labels + membrane_labels\n",
    "\n",
    "# Quick check to make sure we got it right\n",
    "len(sequences) == len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc782dd0",
   "metadata": {},
   "source": [
    "Phew!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aac39c",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9099e7c",
   "metadata": {},
   "source": [
    "Since the data we're loading isn't prepared for us as a machine learning dataset, we'll have to split the data into train and test sets ourselves! We can use sklearn's function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "366147ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(sequences, labels, test_size=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d29b4ed",
   "metadata": {},
   "source": [
    "## Tokenizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02baaf7",
   "metadata": {},
   "source": [
    "All inputs to neural nets must be numerical. The process of converting strings into numerical indices suitable for a neural net is called **tokenization**. For natural language this can be quite complex, as usually the network's vocabulary will not contain every possible word, which means the tokenizer must handle splitting rarer words into pieces, as well as all the complexities of capitalization and unicode characters and so on.\n",
    "\n",
    "With proteins, however, things are very easy. In protein language models, each amino acid is converted to a single token. Every model on `transformers` comes with an associated `tokenizer` that handles tokenization for it, and protein language models are no different. Let's get our tokenizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddbe2b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2b8b0eafde4ea1bd7423f05dbb365c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/40.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f2cf7e70ad4728b9d41a2c206b6807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/93.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73832e01daaa492b83f53c51f546a108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d16be37",
   "metadata": {},
   "source": [
    "Let's try a single sequence to see what the outputs from our tokenizer look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "687386af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 20, 4, 10, 18, 12, 16, 15, 18, 8, 16, 5, 8, 8, 15, 12, 4, 15, 19, 8, 18, 14, 7, 6, 4, 10, 11, 8, 10, 11, 13, 12, 4, 8, 4, 15, 20, 8, 4, 16, 16, 17, 18, 8, 14, 23, 14, 10, 14, 22, 4, 8, 8, 8, 18, 14, 5, 19, 20, 8, 15, 11, 16, 23, 19, 21, 11, 8, 14, 23, 8, 18, 15, 15, 16, 16, 15, 16, 5, 4, 4, 5, 10, 14, 8, 8, 11, 12, 11, 19, 4, 11, 13, 8, 14, 15, 14, 5, 4, 23, 7, 11, 4, 5, 6, 4, 12, 14, 18, 7, 5, 14, 14, 4, 7, 20, 4, 20, 11, 15, 11, 19, 12, 14, 12, 4, 5, 18, 11, 16, 20, 5, 19, 6, 5, 8, 18, 4, 8, 18, 4, 6, 6, 12, 10, 22, 6, 18, 5, 4, 14, 9, 6, 8, 14, 5, 15, 14, 13, 19, 4, 17, 4, 5, 8, 8, 5, 5, 14, 4, 18, 18, 8, 22, 18, 5, 18, 4, 12, 8, 9, 10, 4, 8, 9, 5, 12, 7, 11, 7, 12, 20, 6, 20, 6, 7, 5, 18, 21, 4, 9, 4, 18, 4, 4, 14, 21, 19, 14, 17, 22, 18, 15, 5, 4, 10, 12, 7, 7, 11, 4, 4, 5, 11, 18, 8, 18, 12, 12, 11, 4, 7, 7, 15, 8, 8, 18, 14, 9, 15, 6, 21, 15, 10, 14, 6, 16, 7, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(train_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a719808",
   "metadata": {},
   "source": [
    "This looks good! We can see that our sequence has been converted into `input_ids`, which is the tokenized sequence, and an `attention_mask`. The attention mask handles the case when we have sequences of variable length - in those cases, the shorter sequences are padded with blank \"padding\" tokens, and the attention mask is padded with 0s to indicate that those tokens should be ignored by the model.\n",
    "\n",
    "So now, let's tokenize our whole dataset. Note that we don't need to do anything with the labels, as they're already in the format we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56e26ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = tokenizer(train_sequences)\n",
    "test_tokenized = tokenizer(test_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3681d1",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85089e49",
   "metadata": {},
   "source": [
    "Now we want to turn this data into a dataset that PyTorch can load samples from. We can use the HuggingFace `Dataset` class for this, although if you prefer you can also use `torch.utils.data.Dataset`, at the cost of some more boilerplate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb79ba6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 5652\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "test_dataset = Dataset.from_dict(test_tokenized)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e809e47",
   "metadata": {},
   "source": [
    "This looks good, but we're missing our labels! Let's add those on as an extra column to the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "090acc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 5652\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "test_dataset = test_dataset.add_column(\"labels\", test_labels)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced9aaa8",
   "metadata": {},
   "source": [
    "Looks good! We're ready for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af074a5c",
   "metadata": {},
   "source": [
    "## Model loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab5d70",
   "metadata": {},
   "source": [
    "Next, we want to load our model. Make sure to use exactly the same model as you used when loading the tokenizer, or your model might not understand the tokenization scheme you're using!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc164b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ea0652c62b4acbba8db012f4fddeb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/646 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c430059e6c4eb78e821edd3adc0125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/136M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rocketknight1/esm2_t12_35M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at Rocketknight1/esm2_t12_35M_UR50D and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_labels = max(train_labels + test_labels) + 1  # Add 1 since 0 can be a label\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcba23",
   "metadata": {},
   "source": [
    "These warnings are telling us that the model is discarding some weights that it used for language modelling (the `lm_head`) and adding some weights for sequence classification (the `classifier`). This is exactly what we expect when we want to fine-tune a language model on a sequence classification task!\n",
    "\n",
    "Next, we initialize our `TrainingArguments`. These control the various training hyperparameters, and will be passed to our `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "775cb3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "batch_size = 8\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-localization\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc95d099",
   "metadata": {},
   "source": [
    "Next, we define the metric we will use to evaluate our models and write a `compute_metrics` function. We can load this from the `evaluate` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "471cef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "\n",
    "metric = load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709dcf25",
   "metadata": {},
   "source": [
    "And at last we're ready to initialize our `Trainer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e212b751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/Rocketknight1/esm2_t12_35M_UR50D-finetuned-localization into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32924d0d",
   "metadata": {},
   "source": [
    "You might wonder why we pass along the `tokenizer` when we already preprocessed our data. This is because we will use it one last time to make all the samples we gather the same length by applying padding, which requires knowing the model's preferences regarding padding (to the left or right? with which token?). The `tokenizer` has a pad method that will do all of this right for us, and the `Trainer` will use it. You can customize this part by defining and passing your own `data_collator` which will receive samples like the dictionaries seen above and will need to return a dictionary of tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f7a24c",
   "metadata": {},
   "source": [
    "We can now finetune our model by just calling the `train` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c3cf6da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/PycharmProjects/transformers/src/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5652\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2121\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb: Paste an API key from your profile and hit enter: Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to `offline` in this directory.  Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2121' max='2121' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2121/2121 11:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.258600</td>\n",
       "      <td>0.198971</td>\n",
       "      <td>0.940552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.176900</td>\n",
       "      <td>0.176735</td>\n",
       "      <td>0.943737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.124600</td>\n",
       "      <td>0.180330</td>\n",
       "      <td>0.947452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1884\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to esm2_t12_35M_UR50D-finetuned-localization/checkpoint-707\n",
      "Configuration saved in esm2_t12_35M_UR50D-finetuned-localization/checkpoint-707/config.json\n",
      "Model weights saved in esm2_t12_35M_UR50D-finetuned-localization/checkpoint-707/pytorch_model.bin\n",
      "tokenizer config file saved in esm2_t12_35M_UR50D-finetuned-localization/checkpoint-707/tokenizer_config.json\n",
      "Special tokens file saved in esm2_t12_35M_UR50D-finetuned-localization/checkpoint-707/special_tokens_map.json\n",
      "tokenizer config file saved in esm2_t12_35M_UR50D-finetuned-localization/tokenizer_config.json\n",
      "Special tokens file saved in esm2_t12_35M_UR50D-finetuned-localization/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1884\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to esm2_t12_35M_UR50D-finetuned-localization/checkpoint-1414\n",
      "Configuration saved in esm2_t12_35M_UR50D-finetuned-localization/checkpoint-1414/config.json\n",
      "Model weights saved in esm2_t12_35M_UR50D-finetuned-localization/checkpoint-1414/pytorch_model.bin\n",
      "tokenizer config file saved in esm2_t12_35M_UR50D-finetuned-localization/checkpoint-1414/tokenizer_config.json\n",
      "Special tokens file saved in esm2_t12_35M_UR50D-finetuned-localization/checkpoint-1414/special_tokens_map.json\n",
      "tokenizer config file saved in esm2_t12_35M_UR50D-finetuned-localization/tokenizer_config.json\n",
      "Special tokens file saved in esm2_t12_35M_UR50D-finetuned-localization/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1884\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to esm2_t12_35M_UR50D-finetuned-localization/checkpoint-2121\n",
      "Configuration saved in esm2_t12_35M_UR50D-finetuned-localization/checkpoint-2121/config.json\n",
      "Model weights saved in esm2_t12_35M_UR50D-finetuned-localization/checkpoint-2121/pytorch_model.bin\n",
      "tokenizer config file saved in esm2_t12_35M_UR50D-finetuned-localization/checkpoint-2121/tokenizer_config.json\n",
      "Special tokens file saved in esm2_t12_35M_UR50D-finetuned-localization/checkpoint-2121/special_tokens_map.json\n",
      "tokenizer config file saved in esm2_t12_35M_UR50D-finetuned-localization/tokenizer_config.json\n",
      "Special tokens file saved in esm2_t12_35M_UR50D-finetuned-localization/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from esm2_t12_35M_UR50D-finetuned-localization/checkpoint-2121 (score: 0.947452229299363).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2121, training_loss=0.1768098774522838, metrics={'train_runtime': 678.7337, 'train_samples_per_second': 24.982, 'train_steps_per_second': 3.125, 'total_flos': 2687138428873008.0, 'train_loss': 0.1768098774522838, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec59f4",
   "metadata": {},
   "source": [
    "Nice! After three epochs we have a model accuracy of 94-95%. Note that we didn't do a lot of work to filter the training data or tune hyperparameters for this experiment, and also that we used one of the smallest ESM-2 models. With a larger starting model and more effort to ensure that the training data categories were cleanly separable, accuracy could almost certainly go a lot higher!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3f2ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
