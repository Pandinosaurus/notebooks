{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation\n",
    "! pip install smolagents\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/smolagents.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`smolagents` provides a flexible framework that allows you to use various language models from different providers.\n",
    "This guide will show you how to use different model types with your agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available model types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`smolagents` supports several model types out of the box:\n",
    "1. [InferenceClientModel](https://huggingface.co/docs/smolagents/main/en/reference/models#smolagents.InferenceClientModel): Uses Hugging Face's Inference API to access models\n",
    "2. [TransformersModel](https://huggingface.co/docs/smolagents/main/en/reference/models#smolagents.TransformersModel): Runs models locally using the Transformers library\n",
    "3. [VLLMModel](https://huggingface.co/docs/smolagents/main/en/reference/models#smolagents.VLLMModel): Uses vLLM for fast inference with optimized serving\n",
    "4. [MLXModel](https://huggingface.co/docs/smolagents/main/en/reference/models#smolagents.MLXModel): Optimized for Apple Silicon devices using MLX\n",
    "5. [LiteLLMModel](https://huggingface.co/docs/smolagents/main/en/reference/models#smolagents.LiteLLMModel): Provides access to hundreds of LLMs through LiteLLM\n",
    "6. [LiteLLMRouterModel](https://huggingface.co/docs/smolagents/main/en/reference/models#smolagents.LiteLLMRouterModel): Distributes requests among multiple models\n",
    "7. [OpenAIModel](https://huggingface.co/docs/smolagents/main/en/reference/models#smolagents.OpenAIModel): Provides access to any provider that implements an OpenAI-compatible API\n",
    "8. [AzureOpenAIModel](https://huggingface.co/docs/smolagents/main/en/reference/models#smolagents.AzureOpenAIModel): Uses Azure's OpenAI service\n",
    "9. [AmazonBedrockModel](https://huggingface.co/docs/smolagents/main/en/reference/models#smolagents.AmazonBedrockModel): Connects to AWS Bedrock's API\n",
    "\n",
    "All model classes support passing additional keyword arguments (like `temperature`, `max_tokens`, `top_p`, etc.) directly at instantiation time.\n",
    "These parameters are automatically forwarded to the underlying model's completion calls, allowing you to configure model behavior such as creativity, response length, and sampling strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Google Gemini Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the Google Gemini API documentation (https://ai.google.dev/gemini-api/docs/openai),\n",
    "Google provides an OpenAI-compatible API for Gemini models, allowing you to use the [OpenAIModel](https://huggingface.co/docs/smolagents/main/en/reference/models#smolagents.OpenAIModel)\n",
    "with Gemini models by setting the appropriate base URL.\n",
    "\n",
    "First, install the required dependencies:\n",
    "```bash\n",
    "pip install 'smolagents[openai]'\n",
    "```\n",
    "\n",
    "Then, [get a Gemini API key](https://ai.google.dev/gemini-api/docs/api-key) and set it in your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = <YOUR-GEMINI-API-KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can initialize the Gemini model using the `OpenAIModel` class\n",
    "and setting the `api_base` parameter to the Gemini API base URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import OpenAIModel\n",
    "\n",
    "model = OpenAIModel(\n",
    "    model_id=\"gemini-2.0-flash\",\n",
    "    # Google Gemini OpenAI-compatible API base URL\n",
    "    api_base=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "    api_key=GEMINI_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenRouter Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenRouter provides access to a wide variety of language models through a unified OpenAI-compatible API.\n",
    "You can use the [OpenAIModel](https://huggingface.co/docs/smolagents/main/en/reference/models#smolagents.OpenAIModel) to connect to OpenRouter by setting the appropriate base URL.\n",
    "\n",
    "First, install the required dependencies:\n",
    "```bash\n",
    "pip install 'smolagents[openai]'\n",
    "```\n",
    "\n",
    "Then, [get an OpenRouter API key](https://openrouter.ai/keys) and set it in your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENROUTER_API_KEY = <YOUR-OPENROUTER-API-KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can initialize any model available on OpenRouter using the `OpenAIModel` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import OpenAIModel\n",
    "\n",
    "model = OpenAIModel(\n",
    "    # You can use any model ID available on OpenRouter\n",
    "    model_id=\"openai/gpt-4o\",\n",
    "    # OpenRouter API base URL\n",
    "    api_base=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using xAI's Grok Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xAI's Grok models can be accessed through [LiteLLMModel](https://huggingface.co/docs/smolagents/main/en/reference/models#smolagents.LiteLLMModel).\n",
    "\n",
    "Some models (such as \"grok-4\" and \"grok-3-mini\") don't support the `stop` parameter, so you'll need to use\n",
    "`REMOVE_PARAMETER` to exclude it from API calls.\n",
    "\n",
    "First, install the required dependencies:\n",
    "```bash\n",
    "pip install smolagents[litellm]\n",
    "```\n",
    "\n",
    "Then, [get an xAI API key](https://console.x.ai/) and set it in your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XAI_API_KEY = <YOUR-XAI-API-KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can initialize Grok models using the `LiteLLMModel` class and remove the `stop` parameter if applicable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import LiteLLMModel, REMOVE_PARAMETER\n",
    "\n",
    "# Using Grok-4\n",
    "model = LiteLLMModel(\n",
    "    model_id=\"xai/grok-4\",\n",
    "    api_key=XAI_API_KEY,\n",
    "    stop=REMOVE_PARAMETER,  # Remove stop parameter as grok-4 model doesn't support it\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Or using Grok-3-mini\n",
    "model_mini = LiteLLMModel(\n",
    "    model_id=\"xai/grok-3-mini\",\n",
    "    api_key=XAI_API_KEY,\n",
    "    stop=REMOVE_PARAMETER,  # Remove stop parameter as grok-3-mini model doesn't support it\n",
    "    max_tokens=1000\n",
    ")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
