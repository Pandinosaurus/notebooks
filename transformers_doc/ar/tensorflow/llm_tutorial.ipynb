{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers installation\n",
    "! pip install transformers datasets evaluate accelerate\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ุงูุชูููุฏ ุจุงุณุชุฎุฏุงู ููุงุฐุฌ ุงููุบุงุช ุงููุจูุฑุฉ (LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ุชุนุฏ LLMsุ ุฃู ููุงุฐุฌ ุงููุบุฉ ุงููุจูุฑุฉุ ุงููููู ุงูุฑุฆูุณู ูุฑุงุก ุชูููุฏ ุงููุตูุต. ูุจุงุฎุชุตุงุฑุ ุชุชููู ูู ููุงุฐุฌ ูุญูู ูุจูุฑุฉ ูุณุจูุฉ ุงูุชุฏุฑูุจ ุชู ุชุฏุฑูุจูุง ููุชูุจุค ุจุงููููุฉ ุงูุชุงููุฉ (ุฃูุ ุจุดูู ุฃูุซุฑ ุฏูุฉุ ุงูุฑูุฒ ุงููุบูู) ุจุงููุธุฑ ุฅูู ูุต ูุนูู. ูุธุฑูุง ูุฃููุง ุชุชูุจุฃ ุจุฑูุฒ ูุงุญุฏ ูู ูู ูุฑุฉุ ูุฌุจ ุนููู ุงูููุงู ุจุดูุก ุฃูุซุฑ ุชุนููุฏูุง ูุชูููุฏ ุฌูู ุฌุฏูุฏุฉ ุจุฎูุงู ูุฌุฑุฏ ุงุณุชุฏุนุงุก ุงููููุฐุฌ - ูุฌุจ ุนููู ุฅุฌุฑุงุก ุงูุชูููุฏ ุงูุชููุงุฆู.\n",
    "\n",
    "ุงูุชูููุฏ ุงูุชููุงุฆู ูู ุฅุฌุฑุงุก ููุช ุงูุงุณุชุฏูุงู ุงูุฐู ูุชุถูู ุงุณุชุฏุนุงุก ุงููููุฐุฌ ุจุดูู ูุชูุฑุฑ ุจุงุณุชุฎุฏุงู ูุฎุฑุฌุงุชู ุงูุฎุงุตุฉุ ุจุงููุธุฑ ุฅูู ุจุนุถ ุงููุฏุฎูุงุช ุงูุฃูููุฉ. ูู ๐ค Transformersุ ูุชู ุงูุชุนุงูู ูุน ูุฐุง ุจูุงุณุทุฉ ุฏุงูุฉ `generate()`ุ ูุงูุชู ุชุชููุฑ ูุฌููุน ุงูููุงุฐุฌ ุฐุงุช ุงููุฏุฑุงุช ุงูุชูููุฏูุฉ.\n",
    "\n",
    "ุณููุถุญ ูุฐุง ุงูุจุฑูุงูุฌ ุงูุชุนูููู ููููุฉ:\n",
    "\n",
    "* ุชุชูููุฏ ูุต ุจุงุณุชุฎุฏุงู ูููุฐุฌ ุงููุบุงุช ุงููุจูุฑุฉ (LLM)\n",
    "* ุชุฌูุจ ุงููููุน ูู ุงูุฃุฎุทุงุก ุงูุดุงุฆุนุฉ\n",
    "* ุงูุฎุทูุงุช ุงูุชุงููุฉ ููุณุงุนุฏุชู ูู ุงูุงุณุชูุงุฏุฉ ุงููุตูู ูู LLM ุงูุฎุงุต ุจู\n",
    "\n",
    "ูุจู ุงูุจุฏุกุ ุชุฃูุฏ ูู ุชุซุจูุช ุฌููุน ุงูููุชุจุงุช ุงูุถุฑูุฑูุฉ:\n",
    "\n",
    "```bash\n",
    "pip install transformers bitsandbytes>=0.39.0 -q\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ุชูููุฏ ุงููุต"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ูุฃุฎุฐ ูููุฐุฌ ุงููุบุฉ ุงููุฏุฑุจ ูู [ููุฐุฌุฉ ุงููุบุฉ ุงูุณุจุจูุฉ](https://huggingface.co/docs/transformers/main/ar/tasks/language_modeling) ูุฃุฎุฐ ุชุณูุณููุง ูู ุฑููุฒ ูุตูุฉ ููุฏุฎู ููุนูุฏ ุชูุฒูุน ุงูุงุญุชูุงููุฉ ููุฑูุฒ ุงูุชุงูู.\n",
    "\n",
    "<!-- [GIF 1 -- FWD PASS] -->\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "    <video\n",
    "        style=\"max-width: 90%; margin: auto;\"\n",
    "        autoplay loop muted playsinline\n",
    "        src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_1_1080p.mov\"\n",
    "    ></video>\n",
    "    <figcaption>\"ุงูุชูุจุค ุจุงููููุฉ ุงูุชุงููุฉ ููููุฐุฌ ุงููุบุฉ (LLM)\"</figcaption>\n",
    "</figure>\n",
    "\n",
    "ููุงู ุฌุงูุจ ุจุงูุบ ุงูุฃูููุฉ ูู ุงูุชูููุฏ ุงูุชููุงุฆู ุจุงุณุชุฎุฏุงู LLMs ููู ููููุฉ ุงุฎุชูุงุฑ ุงูุฑูุฒ ุงูุชุงูู ูู ุชูุฒูุน ุงูุงุญุชูุงููุฉ ูุฐุง. ูู ุดูุก ูุณููุญ ุจู ูู ูุฐู ุงูุฎุทูุฉ ุทุงููุง ุฃูู ุชูุชูู ุจุฑูุฒ ููุชูุฑุงุฑ ุงูุชุงูู. ููุฐุง ูุนูู ุฃูู ูููู ุฃู ูููู ุจุณูุทูุง ูุซู ุงุฎุชูุงุฑ ุงูุฑูุฒ ุงูุฃูุซุฑ ุงุญุชูุงููุง ูู ุชูุฒูุน ุงูุงุญุชูุงููุฉ ุฃู ูุนูุฏูุง ูุซู ุชุทุจูู ุนุดุฑุงุช ุงูุชุญููุงุช ูุจู ุฃุฎุฐ ุงูุนููุงุช ูู ุงูุชูุฒูุน ุงููุงุชุฌ.\n",
    "\n",
    "<!-- [GIF 2 -- TEXT GENERATION] -->\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "    <video\n",
    "        style=\"max-width: 90%; margin: auto;\"\n",
    "        autoplay loop muted playsinline\n",
    "        src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov\"\n",
    "    ></video>\n",
    "    <figcaption>\"ุงูุชูููุฏ ุงูุชููุงุฆู ุงููุชุณูุณู\"</figcaption>\n",
    "</figure>\n",
    "\n",
    "ุชุชูุฑุฑ ุงูุนูููุฉ ุงูููุถุญุฉ ุฃุนูุงู ุจุดูู ุชูุฑุงุฑู ุญุชู ูุชู ุงููุตูู ุฅูู ุดุฑุท ุงูุชููู. ูู ุงููุถุน ุงููุซุงููุ ูุญุฏุฏ ุงููููุฐุฌ ุดุฑุท ุงูุชูููุ ูุงูุฐู ูุฌุจ ุฃู ูุชุนูู ุนูุฏ ุฅุฎุฑุงุฌ ุฑูุฒ ููุงูุฉ ุงูุชุณูุณู (`EOS`). ุฅุฐุง ูู ููู ุงูุฃูุฑ ูุฐููุ ูุชููู ุงูุชูููุฏ ุนูุฏ ุงููุตูู ุฅูู ุทูู ุฃูุตู ูุญุฏุฏ ูุณุจููุง.\n",
    "\n",
    "ูู ุงูุถุฑูุฑู ุฅุนุฏุงุฏ ุฎุทูุฉ ุงุฎุชูุงุฑ ุงูุฑูุฒ ูุดุฑุท ุงูุชููู ุจุดูู ุตุญูุญ ูุฌุนู ูููุฐุฌู ูุชุตุฑู ููุง ุชุชููุน ูู ูููุชู. ูููุฐุง ุงูุณุจุจ ูุฏููุง `GenerationConfig` ููู ูุฑุชุจุท ุจูู ูููุฐุฌุ ูุงูุฐู ูุญุชูู ุนูู ูุนููุฉ ุชูููุฏูุฉ ุงูุชุฑุงุถูุฉ ุฌูุฏุฉ ููุชู ุชุญูููู ุฌูุจูุง ุฅูู ุฌูุจ ูุน ูููุฐุฌู.\n",
    "\n",
    "ุฏุนูุง ูุชุญุฏุซ ุนู ุงูููุฏ!\n",
    "\n",
    "\n",
    "<Tip>\n",
    "\n",
    "ุฅุฐุง ููุช ููุชููุง ุจุงูุงุณุชุฎุฏุงู ุงูุฃุณุงุณู ูู LLMุ ูุฅู ูุงุฌูุฉ [`Pipeline`](https://huggingface.co/docs/transformers/main/ar/pipeline_tutorial) ุนุงููุฉ ุงููุณุชูู ูู ููุทุฉ ุงูุทูุงู ุฑุงุฆุนุฉ. ููุน ุฐููุ ุบุงูุจูุง ูุง ุชุชุทูุจ LLMs ููุฒุงุช ูุชูุฏูุฉ ูุซู ุงูุชูููู ูุงูุชุญูู ุงูุฏููู ูู ุฎุทูุฉ ุงุฎุชูุงุฑ ุงูุฑูุฒุ ูุงูุชู ูุชู ุชูููุฐูุง ุจุดูู ุฃูุถู ูู ุฎูุงู `generate()`. ุงูุชูููุฏ ุงูุชููุงุฆู ุจุงุณุชุฎุฏุงู LLMs  ูุณุชููู ุงููุซูุฑ ูู ุงูููุงุฑุฏุฏ ููุฌุจ ุชูููุฐู ุนูู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณููุงุช ููุญุตูู ุนูู ุฃุฏุงุก ูุงูู.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "ุฃููุงูุ ุชุญุชุงุฌ ุฅูู ุชุญููู ุงููููุฐุฌ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ุณุชูุงุญุธ ูุฌูุฏ ูุนุงูููู ูู ุงูุงุณุชุฏุนุงุก `from_pretrained`:\n",
    "\n",
    " - `device_map` ูุถูู ุงูุชูุงู ุงููููุฐุฌ ุฅูู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณููุงุช (GPU) ุงูุฎุงุตุฉ ุจู\n",
    " - `load_in_4bit` ูุทุจู [4-bit dynamic quantization](https://huggingface.co/docs/transformers/main/ar/main_classes/quantization) ูุฎูุถ ูุชุทูุจุงุช ุงูููุงุฑุฏ ุจุดูู ูุจูุฑ\n",
    "\n",
    "ููุงู ุทุฑู ุฃุฎุฑู ูุชููุฆุฉ ูููุฐุฌุ ูููู ูุฐุง ุฎุท ุฃุณุงุณ ุฌูุฏ ููุจุฏุก ุจุงุณุชุฎุฏุงู LLM.\n",
    "\n",
    "ุจุนุฏ ุฐููุ ุชุญุชุงุฌ ุฅูู ูุนุงูุฌุฉ ุฅุฏุฎุงู ุงููุต ุงูุฎุงุต ุจู ุจุงุณุชุฎุฏุงู [ููุฌุฒูุฆ ุงููุบูู](https://huggingface.co/docs/transformers/main/ar/tokenizer_summary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\n",
    "model_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ูุญุชูู ูุชุบูุฑ `model_inputs` ุนูู ุงููุต ุงููุฏุฎู ุจุนุฏ ุชูุณููู ุฅูู ูุญุฏุงุช ูุบููุฉ (tokens)ุ ุจุงูุฅุถุงูุฉ ุฅูู ููุงุน ุงูุงูุชุจุงู. ูู ุญูู ุฃู `generate()` ุชุจุฐู ูุตุงุฑู ุฌูุฏูุง ูุงุณุชูุชุงุฌ ููุงุน ุงูุงูุชุจุงู ุนูุฏูุง ูุง ูุชู ุชูุฑูุฑูุ ููุตู ุจุชูุฑูุฑู ูููุง ุฃููู ุฐูู ููุญุตูู ุนูู ูุชุงุฆุฌ ูุซุงููุฉ.\n",
    "\n",
    "ุจุนุฏ ุชูุณูู ุงููุฏุฎูุงุช ุฅูู ูุญุฏุงุช ูุบููุฉุ ููููู ุงุณุชุฏุนุงุก ุงูุฏุงูุฉ `generate()` ูุฅุฑุฌุงุน ุงููุญุฏุงุช ุงููุบููุฉ ุงููุงุชุฌุฉ. ูุฌุจ ุจุนุฏ ุฐูู ุชุญููู ุงููุญุฏุงุช ุงููููุฏุฉ ุฅูู ูุต ูุจู ุทุจุงุนุชู."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A list of colors: red, blue, green, yellow, orange, purple, pink,'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ุฃุฎูุฑูุงุ ููุณ ุนููู ูุนุงูุฌุฉ ุงููุชุชุงููุงุช ุงููุงุญุฏุฉ ุชูู ุงูุฃุฎุฑู! ููููู ูุนุงูุฌุฉ ูุฌููุนุฉ ูู ุงููุฏุฎูุงุช ุฏูุนุฉ ูุงุญุฏุฉุ ูุงูุชู ุณุชุนูู ุนูู ุชุญุณูู ุงูุฅูุชุงุฌูุฉ ุจุดูู ูุจูุฑ ุจุชูููุฉ ุตุบูุฑุฉ ูู ุฒูู ุงูุงุณุชุฌุงุจุฉ ูุงุณุชููุงู ุงูุฐุงูุฑ. ูู ูุง ุนููู ุงูุชุฃูุฏ ููู ูู  ุชุนุจุฆุฉ ุงููุฏุฎูุงุช ุจุดูู ุตุญูุญ (ุงููุฒูุฏ ุญูู ุฐูู ุฃุฏูุงู)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A list of colors: red, blue, green, yellow, orange, purple, pink,',\n",
       "'Portugal is a country in southwestern Europe, on the Iber']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n",
    "model_inputs = tokenizer(\n",
    "    [\"A list of colors: red, blue\", \"Portugal is\"], return_tensors=\"pt\", padding=True\n",
    ").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ููุฐุง ูู ุดูุก! ูู ุจุถุน ุณุทูุฑ ูู ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉุ ููููู ุชุณุฎูุฑ ููุฉ LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ุงูุฃุฎุทุงุก ุงูุดุงุฆุนุฉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ููุงู ุงูุนุฏูุฏ ูู [ุงุณุชุฑุงุชูุฌูุงุช ุงูุชูููุฏ](https://huggingface.co/docs/transformers/main/ar/generation_strategies)ุ ููู ุจุนุถ ุงูุฃุญูุงู ูุฏ ูุง ุชููู ุงูููู ุงูุงูุชุฑุงุถูุฉ ููุงุณุจุฉ ูุญุงูุชู ุงูุงุณุชุฎุฏุงู. ุฅุฐุง ูู ุชูู ุงูุฅุฎุฑุงุฌ ุงูุฎุงุตุฉ ุจู ูุชูุงููุฉ ูุน ูุง ุชุชููุนูุ ููุฏ ูููุง ุจุฅูุดุงุก ูุงุฆูุฉ ุจุฃูุซุฑ ุงูุฃุฎุทุงุก ุงูุดุงุฆุนุฉ ูููููุฉ ุชุฌูุจูุง."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ุงูุฅุฎุฑุงุฌ ุงููููุฏ ูุตูุฑ ุฌุฏูุง/ุทููู ุฌุฏูุง"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ุฅุฐุง ูู ูุชู ุชุญุฏูุฏ ุงูุนุฏุฏ ุงูุฃูุตู ููุฑููุฒ ูู `GenerationConfig` ุงููููุ `generate` ูุนูุฏ ูุง ูุตู ุฅูู 20 ุฑูุฒูุง ุจุดูู ุงูุชุฑุงุถู. ููุตู ุจุดุฏุฉ ุจุชุนููู `max_new_tokens` ูุฏูููุง ูู ููุงููุฉ `generate` ููุชุญูู ูู ุงูุนุฏุฏ ุงูุฃูุตู ูู ุงูุฑููุฒ ุงูุฌุฏูุฏุฉ ุงูุชู ูููู ุฃู ูุนูุฏูุง. ุถุน ูู ุงุนุชุจุงุฑู ุฃู LLMs (ุจุดูู ุฃูุซุฑ ุฏูุฉุ [ููุงุฐุฌ ูู ุงูุชุดููุฑ ููุท](https://huggingface.co/learn/nlp-course/chapter1/6ุfw=pt)) ุชุนูุฏ ุฃูุถูุง ุงููุฏุฎูุงุช ุงูุฃุตููุฉ ูุฌุฒุก ูู ุงููุงุชุฌ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A sequence of numbers: 1, 2, 3, 4, 5'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer([\"A sequence of numbers: 1, 2\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# By default, the output will contain up to 20 tokens\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting `max_new_tokens` allows you to control the maximum length\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=50)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ูุถุน ุงูุชูููุฏ ุงูุงูุชุฑุงุถู"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ุจุดูู ุงูุชุฑุงุถูุ ููุง ูู ูุชู ุชุญุฏูุฏู ูู `GenerationConfig` ุงููููุ `generate` ูุญุฏุฏ ุงููููุฉ ุงูุฃูุซุฑ ุงุญุชูุงููุง ูู ูู ุฎุทูุฉ ูู ุฎุทูุงุช ุนูููุฉ ุงูุชูููุฏ (ููุฐุง ููุนุฑู ุจุงูุชุดููุฑ ุงูุฌุดุน). ุงุนุชูุงุฏูุง ุนูู ูููุชูุ ูุฏ ูููู ูุฐุง ุบูุฑ ูุฑุบูุจ ูููุ ุชุณุชููุฏ ุงูููุงู ุงูุฅุจุฏุงุนูุฉ ูุซู ุจุฑุงูุฌ ุงูุฏุฑุฏุดุฉ ุฃู ูุชุงุจุฉ ููุงู ุณุชููุฏ ูู ุฃุณููุจ ุงูุนููุฉ ุงูุนุดูุงุฆูุฉ ูู ุงุฎุชูุงุฑ ุงููููุงุชุ ุชูู ูุงุญูุฉ ุฃุฎุฑูุ ูุฅู ุงูููุงู ุงูุชู ุชุนุชูุฏ ุนูู ูุฏุฎูุงุช ูุญุฏุฏุฉ  ูุซู ุชุญููู ุงูุตูุช ุฅูู ูุต ุฃู ุงูุชุฑุฌู ูู ูู ุงูุชุดููุฑ ุงูุฌุดุน. ูู ุจุชูุนูู ุฃุณููุจ ุงูุนููุงุช ุงูุนุดูุงุฆูุฉ ุจุงุณุชุฎุฏุงู `do_sample=True`ุ ูููููู ูุนุฑูุฉ ุงููุฒูุฏ ุญูู ูุฐุง ุงูููุถูุน ูู [ุชุฏูููุฉ ุงููุฏููุฉ](https://huggingface.co/blog/how-to-generate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am a cat. I am a cat. I am a cat. I am a cat'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed or reproducibility -- you don't need this unless you want full reproducibility\n",
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "model_inputs = tokenizer([\"I am a cat.\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# LLM + greedy decoding = repetitive, boring output\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am a cat.  Specifically, I am an indoor-only cat.  I'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With sampling, the output becomes more creative!\n",
    "generated_ids = model.generate(**model_inputs, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ูุดููุฉ ุญุดู ุงููุฏุฎูุงุช ูู ุงูุงุชุฌุงุฉ ุงูุฎุทุฃ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs ูู [ูุนูุงุฑูุงุช ูู ุงูุชุดููุฑ ููุท](https://huggingface.co/learn/nlp-course/chapter1/6ุfw=pt)ุ ููุง ูุนูู ุฃููุง ุชุณุชูุฑ ูู ุงูุชูุฑุงุฑ ุนูู ููุฌู ุงูุฅุฏุฎุงู ุงูุฎุงุต ุจู. ูุฅู ุฌููุน ุงููุฏุฎูุงุช ูุฌุจ ุฃู ุชููู ุจููุณ ุงูุทูู. ูุญู ูุฐู ุงููุณุฃูุฉุ ูุชู ุฅุถุงูุฉ ุฑููุฒ ุญุดู ุฅูู ุงููุฏุฎูุงุช ุงูุฃูุตุฑ. ูุธุฑูุง ูุฃู LLMs  ูุง ุชููู ุงูุชูุงููุง ูุฑููุฒ ุงูุญุดู ูุฐูุ ุฐููุ ูุฌุจ ุชุญุฏูุฏ ุงูุฌุฒุก ุงูููู ูู ุงููุฏุฎู ุงูุฐู ูุฌุจ ุฃู ูุฑูุฒ ุนููู ุงููููุฐุฌุ ููุฐุง ูุชู ุนู ุทุฑูู ูุง ูุณูู ุจู \"ููุงุน ุงูุงูุชุจุงู\". ูุฌุจ ุฃู ูููู ุงูุญุดู ูู ุจุฏุงูุฉ ุงููุฏุฎู (ุงูุญุดู ูู ุงููุณุงุฑ)ุ ูููุณ ูู ููุงูุชู."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1, 2, 33333333333'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tokenizer initialized above has right-padding active by default: the 1st sequence,\n",
    "# which is shorter, has padding on the right side. Generation fails to capture the logic.\n",
    "model_inputs = tokenizer(\n",
    "    [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1, 2, 3, 4, 5, 6,'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With left-padding, it works as expected!\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n",
    "model_inputs = tokenizer(\n",
    "    [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ููุฌู ุบูุฑ ุตุญูุญ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ุชุชููุน ุจุนุถ ููุงุฐุฌ ุงููุบุงุช ุงููุจูุฑุฉ ุนูู ุตูุบุฉ ูุญุฏุฏุฉ ูููุฏุฎูุงุช  ููุนูู ุจุดูู ุตุญูุญ. ุฅุฐุง ูู ูุชู ุงุชุจุงุน ูุฐู ุงูุตูุบุฉุ ูุฅู ุฃุฏุงุก ุงููููุฐุฌ ูุชุฃุซุฑ ุณูุจูุง: ููู ูุฐุง ุงูุชุฏููุฑ ูุฏ ูุง ูููู ูุงุถุญูุง ููุนูุงู. ุชุชููุฑ ูุนูููุงุช ุฅุถุงููุฉ ุญูู ุงูุชูุฌููุ ุจูุง ูู ุฐูู ุงูููุงุฐุฌ ูุงูููุงู ุงูุชู ุชุญุชุงุฌ ุฅูู ุชูุฎู ุงูุญุฐุฑุ ูู [ุงูุฏููู](https://huggingface.co/docs/transformers/main/ar/tasks/prompting). ุฏุนูุง ูุฑู ูุซุงูุงู ุจุงุณุชุฎุฏุงู LLM ููุฏุฑุฏุดุฉุ ูุงูุฐู ูุณุชุฎุฏู [ูุงูุจ ุงูุฏุฑุฏุดุฉ](https://huggingface.co/docs/transformers/main/ar/chat_templating):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm not a thug, but i can tell you that a human cannot eat\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HuggingFaceH4/zephyr-7b-alpha\", device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n",
    ")\n",
    "set_seed(0)\n",
    "prompt = \"\"\"How many helicopters can a human eat in one sitting? Reply as a thug.\"\"\"\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "input_length = model_inputs.input_ids.shape[1]\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=20)\n",
    "print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None, you thug. How bout you try to focus on more useful questions?'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Oh no, it did not follow our instruction to reply as a thug! Let's see what happens when we write\n",
    "# a better prompt and use the right template for this model (through `tokenizer.apply_chat_template`)\n",
    "\n",
    "set_seed(0)\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a thug\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "input_length = model_inputs.shape[1]\n",
    "generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=20)\n",
    "print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see, it followed a proper thug style ๐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ููุงุฑุฏ ุฅุถุงููุฉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ูู ุญูู ุฃู ุนูููุฉ ุงูุชูููุฏ ุงูุชููุงุฆู ุจุณูุทุฉ ูุณุจููุงุ ูุฅู ุงูุงุณุชูุงุฏุฉ ุงููุตูู ูู LLM ุงูุฎุงุต ุจู ูููู ุฃู ุชููู ูููุฉ ุตุนุจุฉ ูุฃู ููุงู ุงูุนุฏูุฏ ูู ุงูุฃุฌุฒุงุก ุงููุชุญุฑูุฉ. ููุฎุทูุงุช ุงูุชุงููุฉ ููุณุงุนุฏุชู ูู ุงูุบูุต ุจุดูู ุฃุนูู ูู ุงุณุชุฎุฏุงู LLM ููููู:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ุงุณุชุฎุฏุงูุงุช ูุชูุฏูุฉ ููุชูููุฏ ูู ููุงุฐุฌ ุงููุบุงุช ุงููุจูุฑุฉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ุฏููู ุญูู ููููุฉ [ุงูุชุญูู ูู ุทุฑู ุงูุชูููุฏ ุงููุฎุชููุฉ](https://huggingface.co/docs/transformers/main/ar/generation_strategies)ุ ูููููุฉ ุฅุนุฏุงุฏ ููู ุชูููู ุงูุชูููุฏุ ูููููุฉ ุจุซ ุงููุงุชุฌุ\n",
    "2. [ุชุณุฑูุน ุชูููุฏ ุงููุต](https://huggingface.co/docs/transformers/main/ar/llm_optims)ุ\n",
    "3.[ููุงูุจ ููุฌูุงุช ููุฏุฑุฏุดุฉ LLMs](https://huggingface.co/docs/transformers/main/ar/chat_\n",
    "4. [ุฏููู ุชุตููู ุงูููุฌู](tasks/prompting);\n",
    "5. ูุฑุฌุน ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช (API)  `GenerationConfig`, `generate()`, ู  [generate-related classes](https://huggingface.co/docs/transformers/main/ar/internal/generation_utils). ูุงูุนุฏูุฏ ูู ุงููุฆุงุช ุงูุฃุฎุฑู ุงููุฑุชุจุทุฉ ุจุนูููุฉ ุงูุชูููุฏ.!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ููุญุงุช ุตุฏุงุฑุฉ ููุงุฐุฌ ุงููุบุงุช ุงููุจูุฑุฉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ููุญุฉ ุตุฏุงุฑุฉ ููุงุฐุฌ ุงููุบุงุช ุงููุจูุฑุฉ ุงูููุชูุญุฉ ุงููุตุฏุฑ (Open LLM Leaderboard): ุชุฑูุฒ ุนูู ุฌูุฏุฉ ุงูููุงุฐุฌ ููุชูุญุฉ ุงููุตุฏุฑ [ุฑุงุจุท ููุญุฉ ุงูุตุฏุงุฑุฉ](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n",
    "2. ููุญุฉ ุตุฏุงุฑุฉ ุฃุฏุงุก ููุงุฐุฌ ุงููุบุงุช ุงููุจูุฑุฉ ุงูููุชูุญุฉ ุงููุตุฏุฑ (Open LLM-Perf Leaderboard): ุชุฑูุฒ ุนูู ุฅูุชุงุฌูุฉ ููุงุฐุฌ ุงููุบุงุช ุงููุจูุฑุฉ [ุฑุงุจุท ููุญุฉ ุงูุตุฏุงุฑุฉ](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ุฒูู ุงูุงุณุชุฌุงุจุฉ ูุงูุฅูุชุงุฌูุฉ ูุงุณุชููุงู ุงูุฐุงูุฑุฉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ุฏููู ุชุญุณูู ููุงุฐุฌ ุงููุบุงุช ุงููุจูุฑุฉ ูู ุญูุซ ุงูุณุฑุนุฉ ูุงูุฐุงูุฑุฉ: ุฏููู ุชุญุณูู ููุงุฐุฌ ุงููุบุงุช ุงููุจูุฑุฉ.\n",
    "2. ุงูุชูููู (Quantization): ุฏููู ุญูู ุชูููุฉ ุงูุชูููู ุงูุชูููู ูุซู ุชูููุชู bitsandbytes ู autogptqุ ูุงูุชู ุชูุถุญ ููููุฉ ุชูููู ูุชุทูุจุงุช ุงูุฐุงูุฑุฉ ุจุดูู ูุจูุฑ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ููุชุจุงุช ูุฑุชุจุทุฉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [`optimum`](https://github.com/huggingface/optimum), ุงูุชุฏุงุฏ ูููุชุจุฉ Transformers ูุนูู ุนูู ุชุญุณูู ุงูุฃุฏุงุก ูุฃุฌูุฒุฉ ูุนููุฉ.\n",
    "2. [`outlines`](https://github.com/outlines-dev/outlines), ููุชุจุฉ ููุชุญูู ูู ุชูููุฏ ุงููุตูุต (ุนูู ุณุจูู ุงููุซุงูุ ูุชูููุฏ ูููุงุช JSON).\n",
    "3. [`SynCode`](https://github.com/uiuc-focal-lab/syncode), ููุชุจุฉ ููุชูููุฏ ุงูููุฌู ุจููุงุนุฏ ุงููุบุฉ ุงูุฎุงููุฉ ูู ุงูุณูุงู (ุนูู ุณุจูู ุงููุซุงูุ JSONุ SQLุ Python).\n",
    "4. [`text-generation-inference`](https://github.com/huggingface/text-generation-inference), ุฎุงุฏู ุฌุงูุฒ ููุฅูุชุงุฌ ูููุงุฐุฌ ุงููุบุงุช ุงููุจูุฑุฉ.\n",
    "5. [`text-generation-webui`](https://github.com/oobabooga/text-generation-webui), ูุงุฌูุฉ ูุณุชุฎุฏู ูุชูููุฏ ุงููุตูุต."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
